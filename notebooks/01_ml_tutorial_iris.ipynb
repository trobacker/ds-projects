{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Tutorial: Iris Classification\n",
    "\n",
    "This notebook demonstrates a complete machine learning workflow using the classic Iris dataset.\n",
    "\n",
    "## Objectives\n",
    "- Load and explore the Iris dataset\n",
    "- Visualize the data to understand patterns\n",
    "- Build and compare multiple classification models\n",
    "- Evaluate model performance\n",
    "- Make predictions on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Dataset\n",
    "\n",
    "The Iris dataset contains measurements of 150 iris flowers from three different species:\n",
    "- Setosa\n",
    "- Versicolor\n",
    "- Virginica\n",
    "\n",
    "Features:\n",
    "- Sepal length (cm)\n",
    "- Sepal width (cm)\n",
    "- Petal length (cm)\n",
    "- Petal width (cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Create a DataFrame for easier manipulation\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df['species'] = iris.target\n",
    "df['species_name'] = df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "print(\"Class distribution:\")\n",
    "print(df['species_name'].value_counts())\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "df['species_name'].value_counts().plot(kind='bar', color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "plt.title('Distribution of Iris Species')\n",
    "plt.xlabel('Species')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot to visualize relationships between features\n",
    "sns.pairplot(df, hue='species_name', palette='Set2', diag_kind='kde')\n",
    "plt.suptitle('Pairplot of Iris Features', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = df[iris.feature_names].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, fmt='.2f')\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for each feature\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Distribution of Features by Species', fontsize=16)\n",
    "\n",
    "for idx, feature in enumerate(iris.feature_names):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    sns.boxplot(data=df, x='species_name', y=feature, ax=ax, palette='Set2')\n",
    "    ax.set_title(feature.replace(' (cm)', '').title())\n",
    "    ax.set_xlabel('Species')\n",
    "    ax.set_ylabel('Measurement (cm)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and target\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split into training and testing sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Standardize features (important for some algorithms)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nData prepared and standardized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Multiple Models\n",
    "\n",
    "We'll train and compare four different classification algorithms:\n",
    "1. Logistic Regression\n",
    "2. Decision Tree\n",
    "3. Random Forest\n",
    "4. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=200),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=5),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'LightGBM': LGBMClassifier(random_state=42, n_estimators=100, verbose=-1)\n",
    "}\n",
    "\n",
    "# Train models and store results\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Use scaled data for Logistic Regression, original for tree-based models\n",
    "    if name == 'Logistic Regression':\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        # Cross-validation score\n",
    "        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        # Cross-validation score\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'predictions': y_pred,\n",
    "        'accuracy': accuracy,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std()\n",
    "    }\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Cross-validation Score: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model accuracies\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Test Accuracy': [results[m]['accuracy'] for m in results.keys()],\n",
    "    'CV Mean': [results[m]['cv_mean'] for m in results.keys()],\n",
    "    'CV Std': [results[m]['cv_std'] for m in results.keys()]\n",
    "})\n",
    "\n",
    "comparison_df = comparison_df.sort_values('Test Accuracy', ascending=False).reset_index(drop=True)\n",
    "print(\"Model Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, comparison_df['Test Accuracy'], width, label='Test Accuracy', color='#4ECDC4')\n",
    "bars2 = ax.bar(x + width/2, comparison_df['CV Mean'], width, label='CV Mean', color='#45B7D1')\n",
    "\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['Model'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim([0.9, 1.02])  # Extra headroom for labels at top\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.002,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation for the best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_predictions = results[best_model_name]['predictions']\n",
    "\n",
    "print(f\"\\nDetailed Evaluation for {best_model_name}:\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, best_predictions, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for best model\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=iris.target_names, \n",
    "            yticklabels=iris.target_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Make Predictions on New Data\n",
    "\n",
    "Let's use our best model to make predictions on hypothetical new iris flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some example new data\n",
    "new_samples = np.array([\n",
    "    [5.1, 3.5, 1.4, 0.2],  # Likely Setosa\n",
    "    [6.7, 3.0, 5.2, 2.3],  # Likely Virginica\n",
    "    [5.9, 3.0, 4.2, 1.5],  # Likely Versicolor\n",
    "])\n",
    "\n",
    "# Get the best model\n",
    "best_model = results[best_model_name]['model']\n",
    "\n",
    "# Make predictions (scale if using Logistic Regression)\n",
    "if best_model_name == 'Logistic Regression':\n",
    "    new_samples_scaled = scaler.transform(new_samples)\n",
    "    predictions = best_model.predict(new_samples_scaled)\n",
    "    probabilities = best_model.predict_proba(new_samples_scaled)\n",
    "else:\n",
    "    predictions = best_model.predict(new_samples)\n",
    "    probabilities = best_model.predict_proba(new_samples)\n",
    "\n",
    "# Display predictions\n",
    "print(\"Predictions for New Samples:\\n\")\n",
    "for i, (sample, pred, probs) in enumerate(zip(new_samples, predictions, probabilities)):\n",
    "    print(f\"Sample {i+1}: {sample}\")\n",
    "    print(f\"  Predicted Species: {iris.target_names[pred]}\")\n",
    "    print(f\"  Probabilities: {dict(zip(iris.target_names, probs.round(3)))}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we:\n",
    "\n",
    "1. \u2705 Loaded and explored the Iris dataset\n",
    "2. \u2705 Visualized the data using various plots\n",
    "3. \u2705 Prepared the data by splitting and scaling\n",
    "4. \u2705 Trained four different classification models\n",
    "5. \u2705 Evaluated and compared model performance\n",
    "6. \u2705 Made predictions on new data\n",
    "\n",
    "**Key Findings:**\n",
    "- All models achieved high accuracy (>90%) on this well-separated dataset\n",
    "- Petal measurements (length and width) are stronger predictors than sepal measurements\n",
    "- Tree-based models (Random Forest, LightGBM) performed particularly well\n",
    "\n",
    "**Next Steps:**\n",
    "- Try different hyperparameters with `GridSearchCV` or `RandomizedSearchCV`\n",
    "- Experiment with feature engineering\n",
    "- Apply these techniques to more complex datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}